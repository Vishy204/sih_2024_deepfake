{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cd783b-57c8-44d7-8c71-b36e69a6b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Image dimensions standardisation\n",
    "image_dimensions={'height':256,'width':256,'channels':3}\n",
    "\n",
    "def extract_mfcc_features(audio_path, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    try:\n",
    "        audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "   \n",
    "\n",
    "def analyze_audio(input_audio_path):\n",
    "    scaler_filename = \"./models/scaler.pkl\"\n",
    "    model_filename = \"models/svm_model_best.pkl\"\n",
    "    svm_classifier = joblib.load(model_filename)\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "    if not os.path.exists(input_audio_path):\n",
    "        print(\"Error: The specified file does not exist.\")\n",
    "        return\n",
    "    elif not input_audio_path.lower().endswith(\".wav\"):\n",
    "        print(\"Error: The specified file is not a .wav file.\")\n",
    "        return\n",
    "\n",
    "    mfcc_features = extract_mfcc_features(input_audio_path)\n",
    "\n",
    "    if mfcc_features is not None:\n",
    "        mfcc_features_scaled = scaler.transform(mfcc_features.reshape(1, -1))\n",
    "        prediction = svm_classifier.predict(mfcc_features_scaled)\n",
    "        if prediction[0] == 0:\n",
    "            return \"The input audio is classified as genuine.\"\n",
    "        else:\n",
    "            return \"The input audio is classified as deepfake.\" \n",
    "    else:\n",
    "        return \"Error: Unable to process the input audio.\"\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Check if each class has at least two samples\n",
    "    if len(X_genuine) < 2 or len(X_deepfake) < 2:\n",
    "        print(\"Each class should have at least two samples for stratified splitting.\")\n",
    "        print(\"Combining both classes into one for training.\")\n",
    "        X = np.vstack((X_genuine, X_deepfake))\n",
    "        y = np.hstack((y_genuine, y_deepfake))\n",
    "    else:\n",
    "        X = np.vstack((X_genuine, X_deepfake))\n",
    "        y = np.hstack((y_genuine, y_deepfake))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67004e8c-fab3-4a0e-87b0-e367cfb37293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "#Wrapper class for model\n",
    "class Classifier:\n",
    "    def __init__():\n",
    "        self.model = 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x.size == 0:\n",
    "            return []\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        return self.model.train_on_batch(x, y)\n",
    "\n",
    "    def get_accuracy(self, x, y):\n",
    "        return self.model.test_on_batch(x, y)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "class Meso4(Classifier):\n",
    "  #Initiialising the class\n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.model = self.init_model()\n",
    "        optimizer = Adam(learning_rate = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "    def init_model(self):\n",
    "      #Input Layer\n",
    "        x = Input(shape = (image_dimensions['height'], image_dimensions['width'], image_dimensions['channels']))\n",
    "\n",
    "#4 convolutional blocks\n",
    "        x1 = Conv2D(8, (3, 3), padding='same', activation = 'relu')(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "\n",
    "        x2 = Conv2D(8, (5, 5), padding='same', activation = 'relu')(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "\n",
    "        x3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n",
    "        x3 = BatchNormalization()(x3)\n",
    "        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "\n",
    "        x4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "\n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLU(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation = 'sigmoid')(y)\n",
    "\n",
    "        return Model(inputs = x, outputs = y)\n",
    "\n",
    "#Initializing model and loading weights\n",
    "model=Meso4()\n",
    "model.load(\"models/Meso4_DF.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b3a71e-e33a-4df7-8089-060693dd4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image, target_size=(256, 256)):\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the image to match model input\n",
    "    img = cv2.resize(img, target_size)\n",
    "\n",
    "    # Normalize the image\n",
    "    img = img.astype('float32') / 255.0\n",
    "\n",
    "    # Expand dimensions to match model input shape\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    return img\n",
    "\n",
    "# Function to analyze the video and make predictions\n",
    "def analyze_video(video_path, model):\n",
    "    sum_predictions = 0\n",
    "    list_of_predictions = []\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess_image(frame)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(preprocessed_frame)\n",
    "\n",
    "        # Display predictions on the frame\n",
    "        prediction_value = float(predictions[0][0])\n",
    "        list_of_predictions.append(prediction_value)\n",
    "\n",
    "\n",
    "        # Display the frame\n",
    "        # cv2.imshow('Video', frame)  # Uncomment to display video with prediction\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Calculate the average of the predictions\n",
    "    for i in list_of_predictions:\n",
    "        sum_predictions+=i\n",
    "    average_prediction = sum_predictions / len(list_of_predictions)\n",
    "\n",
    "    # Determine if the video is real or fake based on the average prediction\n",
    "    return average_prediction,list(list_of_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccbf1ad-6126-4a6f-bfdd-1a89f19fa883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct\n",
    "from skimage.feature import local_binary_pattern\n",
    "import os\n",
    "import json\n",
    "import pymediainfo\n",
    "from pymediainfo import MediaInfo\n",
    "import subprocess\n",
    "\n",
    "def analyze_metadata(video_path):\n",
    "    \n",
    "    \"\"\"Extract and display basic metadata from a video file using OpenCV.\"\"\"\n",
    "    try:\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # Check if the video opened successfully\n",
    "        if not cap.isOpened():\n",
    "            return \"Error opening video file.\"\n",
    "        \n",
    "        # Get video metadata\n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        codec = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = frame_count / fps\n",
    "        \n",
    "        # Convert codec to a readable format\n",
    "        codec_str = ''.join([chr((codec >> 8 * i) & 0xFF) for i in range(4)])\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "        return (f\"Video Duration: {duration:.2f} seconds\\n\"\n",
    "                f\"Video Resolution: {int(width)}x{int(height)}\\n\"\n",
    "                f\"Video Codec: {codec_str}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing metadata: {str(e)}\"\n",
    "\n",
    "def video_deepfake_detector(video_path):\n",
    "    \"\"\"Run comprehensive checks to detect potential video deepfakes.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    results.append(analyze_metadata(video_path))\n",
    "\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f72de87f-fbd5-49d3-ac35-16a9cbad844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import contextlib\n",
    "\n",
    "def analyze_wav_metadata(audio_path):\n",
    "    \"\"\"Extract and display basic metadata from a .wav audio file.\"\"\"\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(audio_path, 'r')) as wav_file:\n",
    "            # Extract basic metadata\n",
    "            params = wav_file.getparams()\n",
    "            duration = params.nframes / params.framerate\n",
    "            metadata = {\n",
    "                'Channels': params.nchannels,\n",
    "                'Sample Width': params.sampwidth,\n",
    "                'Frame Rate': params.framerate,\n",
    "                'Number of Frames': params.nframes,\n",
    "                'Duration (seconds)': duration\n",
    "            }\n",
    "\n",
    "        metadata_str = \"\\n\".join(f\"{key}: {value}\" for key, value in metadata.items())\n",
    "        return f\"WAV Audio Metadata:\\n{metadata_str}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing metadata: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e88ad93-fdaa-4151-91de-5f2b368120b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "import imutils\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to track eyelid movement for irregularities\n",
    "def analyze_blink_smoothness(ear_values, threshold=0.05):\n",
    "    differences = np.diff(ear_values)  # Get the differences between consecutive EAR values\n",
    "    irregular_movements = sum(abs(diff) > threshold for diff in differences)  # Count large differences\n",
    "    return irregular_movements\n",
    "\n",
    "# Main function to process the video and return results\n",
    "def process_video(video_path):\n",
    "    # Load the pre-trained facial landmark detector and shape predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "    # Eye aspect ratio threshold for blink detection\n",
    "    EYE_AR_THRESH = 0.25\n",
    "    EYE_AR_CONSEC_FRAMES = 3\n",
    "    BLINK_IRREGULARITY_THRESH = 3  # Customize based on experiments\n",
    "\n",
    "    # Initialize counters and variables\n",
    "    blink_counter = 0\n",
    "    total_blinks = 0\n",
    "    irregular_blinks = 0\n",
    "    blink_started = False\n",
    "    ear_values = []\n",
    "\n",
    "    # Start video capture\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Grab the indexes of the facial landmarks for the left and right eye\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "    frame_to_return = None  # Initialize the frame to return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=450)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            # Visualize the landmarks for the eyes by drawing circles\n",
    "            for (x, y) in leftEye:\n",
    "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # Draw circles for the left eye\n",
    "            for (x, y) in rightEye:\n",
    "                cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # Draw circles for the right eye\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                blink_counter += 1\n",
    "                ear_values.append(ear)  # Track EAR during blink\n",
    "                if not blink_started:\n",
    "                    blink_started = True\n",
    "                    ear_values = [ear]  # Start tracking from the first frame of blink\n",
    "            else:\n",
    "                if blink_counter >= EYE_AR_CONSEC_FRAMES:\n",
    "                    total_blinks += 1\n",
    "\n",
    "                    # Check for irregularities in blink smoothness\n",
    "                    irregularities = analyze_blink_smoothness(ear_values)\n",
    "                    if irregularities > BLINK_IRREGULARITY_THRESH:\n",
    "                        irregular_blinks += 1\n",
    "\n",
    "                blink_counter = 0\n",
    "                blink_started = False\n",
    "\n",
    "        # Store the current frame for returning later\n",
    "        frame_to_return = frame\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "    # If a frame was processed, convert it to base64\n",
    "    if frame_to_return is not None:\n",
    "        # Add text for total blinks and irregular blinks to the frame\n",
    "        cv2.putText(frame_to_return, f\"Total Blinks: {total_blinks}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(frame_to_return, f\"Irregular Blinks: {irregular_blinks}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Convert the frame to a PIL image and then to a byte buffer\n",
    "        _, buffer = cv2.imencode('.png', frame_to_return)\n",
    "        pil_image = Image.open(io.BytesIO(buffer))\n",
    "        \n",
    "        img_buffer = io.BytesIO()\n",
    "        pil_image.save(img_buffer, format=\"PNG\")\n",
    "        img_buffer.seek(0)\n",
    "        \n",
    "        # Encode the image as base64 for return or transmission\n",
    "        img_base64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "        # Return the base64 image and blink information\n",
    "        return {\"image_base64\": img_base64, \"total_blinks\": total_blinks, \"irregular_blinks\": irregular_blinks}\n",
    "    else:\n",
    "        # Return default values if no frame was processed\n",
    "        return {\"image_base64\": None, \"total_blinks\": total_blinks, \"irregular_blinks\": irregular_blinks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ca34fa-8831-4a69-9ebb-2aa03b6e9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelBinarizer from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RidgeClassifier from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2  # OpenCV for video processing\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from scipy.fftpack import dct\n",
    "with open('models/frequencymodel.pkl', 'rb') as f:\n",
    "    ridge_clf = pickle.load(f)\n",
    "\n",
    "\n",
    "def extract_dct(img):\n",
    "    img = dct(img, type=2, norm=\"ortho\", axis=0)\n",
    "    img = dct(img, type=2, norm=\"ortho\", axis=1)\n",
    "    img = np.abs(img)\n",
    "    img += 1e-13\n",
    "    img = np.log(img)\n",
    "    img -= np.mean(img)\n",
    "    img /= np.std(img)\n",
    "    \n",
    "    # Return both flattened features and DCT image\n",
    "    return img.flatten(), img\n",
    "\n",
    "def predict_video(video_path, model, image_size=(128, 128)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    predictions = []\n",
    "    dct_images = []\n",
    "    frames = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_resized = resize(frame, image_size, anti_aliasing=True)\n",
    "        frame_gray = rgb2gray(frame_resized)\n",
    "\n",
    "        dct_features, dct_image = extract_dct(frame_gray)\n",
    "        prediction = model.predict([dct_features])[0]\n",
    "        probability = model.decision_function([dct_features])[0]\n",
    "\n",
    "        predictions.append((prediction, probability))\n",
    "        dct_images.append(dct_image)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    highest_prob_index = np.argmax([prob for _, prob in predictions])\n",
    "    most_probable_frame = frames[highest_prob_index]\n",
    "    most_probable_dct_image = dct_images[highest_prob_index]\n",
    "\n",
    "    # Create an in-memory byte buffer for the frame and DCT plot\n",
    "    frame_buffer = io.BytesIO()\n",
    "    dct_buffer = io.BytesIO()\n",
    "\n",
    "    # Save the frame to the buffer\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cv2.cvtColor(most_probable_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(frame_buffer, format='png')\n",
    "    frame_buffer.seek(0)  # Move the pointer to the start of the buffer\n",
    "\n",
    "    # Save the DCT plot to the buffer\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(most_probable_dct_image, cmap='inferno')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(dct_buffer, format='png')\n",
    "    dct_buffer.seek(0)\n",
    "\n",
    "    # Encode the buffers to base64 for sending over HTTP\n",
    "    frame_base64 = base64.b64encode(frame_buffer.read()).decode('utf-8')\n",
    "    dct_base64 = base64.b64encode(dct_buffer.read()).decode('utf-8')\n",
    "\n",
    "    # Return the prediction result and the base64 images\n",
    "    return {\n",
    "        \"prediction\": int(np.round(np.mean([pred for pred, _ in predictions]))),\n",
    "        \"frame_base64\": frame_base64,\n",
    "        \"dct_base64\": dct_base64\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b8d943-5565-484c-9bf1-9425db127f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:29:15.098540: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:29:15.099526: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:29:15.099957: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-19 12:29:15.140189: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-19 12:29:15.150040: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:29:15.150504: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:29:15.150964: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-19 12:29:15.268231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:29:15.268687: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:29:15.269047: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-19 12:29:15.307431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-19 12:29:15.317320: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:29:15.317684: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:29:15.318077: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import SpatialDropout3D, Input, Conv3D, BatchNormalization, Activation, MaxPooling3D, Bidirectional, LSTM, Dense, TimeDistributed, ZeroPadding3D, Flatten, Dropout, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "a=\"\"\n",
    "# Load dLib's pre-trained face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def load_video(path: str) -> List[float]:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    target_size = (60, 40)  # Define a fixed size for the cropped lip region (height, width)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale (dLib expects grayscale images)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = detector(gray_frame)\n",
    "\n",
    "        for face in faces:\n",
    "            # Get the landmarks/parts for the face\n",
    "            landmarks = predictor(gray_frame, face)\n",
    "\n",
    "            # Extract the coordinates of the lips (points 48-67)\n",
    "            lip_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)]\n",
    "\n",
    "            # Create a bounding box around the lips\n",
    "            x_coords, y_coords = zip(*lip_points)\n",
    "            min_x, max_x = min(x_coords), max(x_coords)\n",
    "            min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "            # Crop the lip region from the frame\n",
    "            lip_region = frame[min_y:max_y, min_x:max_x]\n",
    "\n",
    "            # Resize the lip region to the target size\n",
    "            lip_region_resized = cv2.resize(lip_region, target_size)\n",
    "\n",
    "            # Convert to grayscale and append to frames\n",
    "            lip_region_gray = tf.image.rgb_to_grayscale(lip_region_resized)\n",
    "            frames.append(lip_region_gray)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames - mean), tf.float32) / std\n",
    "\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(path: str):\n",
    "    path = bytes.decode(path.numpy())\n",
    "    frames = load_video(path)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def mappable_function(path:str) ->List[str]:\n",
    "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
    "    return result\n",
    "\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "inputs = Input(shape=(75, 40, 60, 1))\n",
    "\n",
    "    # Convolutional layers with BatchNormalization and SpatialDropout3D\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(inputs)\n",
    "x = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(x)\n",
    "x = Activation('relu', name='actv1')(x)\n",
    "x = BatchNormalization(name='batc1')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_1')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(x)\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(x)\n",
    "x = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(x)\n",
    "x = Activation('relu', name='actv2')(x)\n",
    "x = BatchNormalization(name='batc2')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_2')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(x)\n",
    "x = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(x)\n",
    "x = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(x)\n",
    "x = Activation('relu', name='actv3')(x)    \n",
    "x = BatchNormalization(name='batc3')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_3')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(x)\n",
    "\n",
    "    # Reshape for RNN layers\n",
    "x = TimeDistributed(Reshape((-1,)), name='time_distributed_1')(x)\n",
    "    # RNN layers\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal  , name='gru1'), merge_mode='concat')(x)\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal , name='gru2'), merge_mode='concat')(x)\n",
    "\n",
    "    # Dense and Activation layers\n",
    "x = Dense(41, kernel_initializer='he_normal', name='dense1')(x)\n",
    "x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "model_lip = tf.keras.Model(inputs, x)\n",
    "\n",
    "model_lip.load_weights(\"./models/dlib3_lipnet_model.h5\")\n",
    "model_lip.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)\n",
    "\n",
    "# Example model prediction line\n",
    "def model_predict(sample):\n",
    "    # Assuming your model expects input of shape (1, 75, 40, 60, 1)\n",
    "    yhat = model_lip.predict(tf.expand_dims(sample, axis=0))\n",
    "    return yhat\n",
    "\n",
    "# Function to split video into chunks of (75, 40, 60, 1)\n",
    "def split_video_into_chunks(video, chunk_size=75):\n",
    "    \"\"\"\n",
    "    Splits the video into chunks of shape (75, 40, 60, 1).\n",
    "    If the last chunk has fewer than 75 frames, it will be padded with zeros.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_frames = video.shape[0]  # The first dimension is the number of frames (x)\n",
    "\n",
    "    # Iterate through the video in steps of 75 frames\n",
    "    for i in range(0, total_frames, chunk_size):\n",
    "        chunk = video[i:i + chunk_size]\n",
    "        \n",
    "        # If the chunk has fewer than 75 frames, pad it\n",
    "        if chunk.shape[0] < chunk_size:\n",
    "            padding = np.zeros((chunk_size - chunk.shape[0], 40, 60, 1))\n",
    "            chunk = np.concatenate((chunk, padding), axis=0)\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Function to process video through the model\n",
    "def process_video_through_model(video):\n",
    "    \"\"\"\n",
    "    Takes a video of shape (x, 40, 60, 1) where x is the number of frames, splits it into\n",
    "    chunks of shape (75, 40, 60, 1), and runs each chunk through the model.\n",
    "    \"\"\"\n",
    "    # Split the video into chunks of (75, 40, 60, 1)\n",
    "    video_chunks = split_video_into_chunks(video)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Run each chunk through the model\n",
    "    for chunk in video_chunks:\n",
    "        print(chunk.shape)\n",
    "        yhat = model_lip.predict(tf.expand_dims(chunk, axis=0))\n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()\n",
    "        tensor = [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]\n",
    "        string_value = tensor[0].numpy().decode(\"utf-8\")\n",
    "        predictions.append(string_value)\n",
    "    \n",
    "    # Concatenate all predictions into a single string\n",
    "    full_prediction_string = ' '.join(predictions)\n",
    "    \n",
    "    return full_prediction_string\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdf98a-f505-4fa5-ba72-42a95e2d6eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e2a528-5930-4544-8a53-55fec5eb2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "def convert_video_to_wav(video_file, output_audio=\"extracted_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Convert a video file to .wav audio format using moviepy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the video file\n",
    "        video = VideoFileClip(video_file)\n",
    "\n",
    "        # Extract audio and write it to a .wav file\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(output_audio, codec='pcm_s16le')  # 'pcm_s16le' for .wav format\n",
    "        print(f\"Audio extracted and saved as {output_audio}\")\n",
    "        return output_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video to audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    \"\"\"\n",
    "    Convert an audio file (wav format) to text using Google Speech Recognition.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            print(\"Recognizing audio...\")\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "            # Convert speech to text using Google's Speech-to-Text\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(\"Transcribed Text: \", text)\n",
    "            return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error with Google Speech Recognition service: {e}\")\n",
    "        return None\n",
    "\n",
    "def video_to_text(video_file):\n",
    "    \"\"\"\n",
    "    Complete process: Convert a video file to text by extracting audio and\n",
    "    applying speech-to-text conversion.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the video to wav format\n",
    "    audio_file = convert_video_to_wav(video_file)\n",
    "\n",
    "    if audio_file:\n",
    "        # Step 2: Convert the extracted audio to text\n",
    "        text = audio_to_text(audio_file)\n",
    "        return text\n",
    "    else:\n",
    "        print(\"Audio extraction failed.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94b301-3285-4318-a1c6-5ca64e96373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "\n",
      "INFO:     Started server process [21704]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:30:35.635020: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "(75, 40, 60, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:30:44.822741: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:30:44.823436: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:30:44.823902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-19 12:30:44.882518: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-09-19 12:30:44.895408: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:30:44.895862: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:30:44.896376: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-19 12:30:44.964920: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:30:44.965474: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:30:44.966117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-09-19 12:30:45.021778: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 490ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:30:45.035312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-09-19 12:30:45.035869: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-09-19 12:30:45.036431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Error converting video to audio: 'NoneType' object has no attribute 'write_audiofile'\n",
      "Audio extraction failed.\n",
      "INFO:     127.0.0.1:57837 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59500 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59503 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Error converting video to audio: 'NoneType' object has no attribute 'write_audiofile'\n",
      "Audio extraction failed.\n",
      "INFO:     127.0.0.1:59988 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved as extracted_audio.wav\n",
      "Recognizing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  hello this is a deep fake\n",
      "INFO:     127.0.0.1:61639 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:62768 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved as extracted_audio.wav\n",
      "Recognizing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  hello this is a deep fake\n",
      "INFO:     127.0.0.1:64138 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/Users/vishwajithp/anaconda3/envs/TensorFlow/lib/python3.11/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.3.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:49313 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "(75, 40, 60, 1)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved as extracted_audio.wav\n",
      "Recognizing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  hello this is a deep fake\n",
      "INFO:     127.0.0.1:50334 - \"POST /upload HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "import os\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "UPLOAD_DIRECTORY = \"./uploaded_files\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(UPLOAD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3001\"],  # React dev server URL\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def upload_file(video: UploadFile = File(None), audio: UploadFile = File(None)):\n",
    "    result = None\n",
    "    random_array = []\n",
    "    metadata=None\n",
    "    frame_base64=None\n",
    "    dct_base64=None\n",
    "    image_base64=None\n",
    "    total_blinks=None\n",
    "    irregular_blinks=None\n",
    "    full_prediction_string=None\n",
    "    transcribed_text = None\n",
    "    similarity=None\n",
    "    micro=None\n",
    "    freq=None\n",
    "    gaze=None\n",
    "    lip=None\n",
    "    mfcc1=None\n",
    "    mfcc2=None\n",
    "    mfcc3=None\n",
    "    mfcc1_64=None\n",
    "    mfcc2_64=None\n",
    "    prediction=None\n",
    "    mfcc3_64=None\n",
    "    \n",
    "    if video:\n",
    "        video_path = os.path.join(UPLOAD_DIRECTORY, video.filename)\n",
    "        with open(video_path, \"wb\") as buffer:\n",
    "            buffer.write(await video.read())\n",
    "\n",
    "        # Call the video analysis function\n",
    "        metadata=video_deepfake_detector(video_path)\n",
    "        result, random_array = analyze_video(video_path,model)\n",
    "        if result>0.5:\n",
    "            micro=\"The Microexpressions are human-like and this is not a deepfake.\"\n",
    "            freq=\"The frequency-analysis (Discrete Cosine Transform) tells us that it is not a deepfake\"\n",
    "        else:\n",
    "            micro=\"The Microexpressions are not human-like and this is a deepfake.\"\n",
    "            freq=\"The frequency-analysis (Discrete Cosine Transform) tells us that it is a deepfake\"\n",
    "            \n",
    "        \n",
    "        analysis_result = predict_video(video_path, ridge_clf)\n",
    "        prediction = analysis_result['prediction']\n",
    "        frame_base64 = analysis_result['frame_base64']\n",
    "        dct_base64 = analysis_result['dct_base64']\n",
    "\n",
    "\n",
    "        eye_result=process_video(video_path)\n",
    "        image_base64 = eye_result[\"image_base64\"]\n",
    "        total_blinks = eye_result[\"total_blinks\"]\n",
    "        irregular_blinks = eye_result[\"irregular_blinks\"]\n",
    "        if irregular_blinks>1:\n",
    "            gaze=\"The blinks are irregular which suggest that it is a deepfake.\"\n",
    "        else:\n",
    "            gaze=\"The blinks are regular which suggests that it is not a deepfake\"\n",
    "\n",
    "\n",
    "        video = load_data(tf.convert_to_tensor(video_path))\n",
    "        full_prediction_string = process_video_through_model(video)\n",
    "\n",
    "        transcribed_text = video_to_text(video_path)\n",
    "        similarity = fuzz.ratio(full_prediction_string, transcribed_text)\n",
    "        if similarity>50:\n",
    "            lip=\"The synchnorisation Score is high enough for it to be not a deepfake.\"\n",
    "        else:\n",
    "            lip=\"The synchronisation Score is low so it is deepfake\"\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "    if audio:\n",
    "        audio_path = os.path.join(UPLOAD_DIRECTORY, audio.filename)\n",
    "        with open(audio_path, \"wb\") as buffer:\n",
    "            buffer.write(await audio.read())\n",
    "        # Create in-memory buffers for images\n",
    "         # Load audio and plot data\n",
    "        real_ad, real_sr = librosa.load(audio_path)\n",
    "\n",
    "        # Create in-memory buffers for images\n",
    "        mfcc1 = io.BytesIO()\n",
    "        mfcc2 = io.BytesIO()\n",
    "        mfcc3 = io.BytesIO()\n",
    "\n",
    "        # Plot waveform and save as image\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(real_ad)\n",
    "        plt.title(\"Audio Data\")\n",
    "        plt.tight_layout()  # Ensure the layout is tight to avoid clipping\n",
    "        plt.savefig(mfcc1, format='png', bbox_inches='tight')  # Save plot to buffer\n",
    "        plt.close()  # Close the figure\n",
    "        mfcc1.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Plot Mel spectrogram\n",
    "        real_mel_spect = librosa.feature.melspectrogram(y=real_ad, sr=real_sr)\n",
    "        real_mel_spect = librosa.power_to_db(real_mel_spect, ref=np.max)\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(real_mel_spect, y_axis=\"mel\", x_axis=\"time\")\n",
    "        plt.title(\"Audio Mel Spectrogram\")\n",
    "        plt.colorbar(format=\"%+2.0f dB\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(mfcc2, format='png', bbox_inches='tight')\n",
    "        plt.close()  # Close the figure\n",
    "        mfcc2.seek(0)\n",
    "\n",
    "        # Plot MFCCs\n",
    "        real_mfccs = librosa.feature.mfcc(y=real_ad, sr=real_sr)\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(real_mfccs, sr=real_sr, x_axis=\"time\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Audio MFCCs\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(mfcc3, format='png', bbox_inches='tight')\n",
    "        plt.close()  # Close the figure\n",
    "        mfcc3.seek(0)\n",
    "\n",
    "        # Convert to base64 strings\n",
    "        mfcc1_64 = base64.b64encode(mfcc1.read()).decode('utf-8')\n",
    "        mfcc2_64 = base64.b64encode(mfcc2.read()).decode('utf-8')\n",
    "        mfcc3_64 = base64.b64encode(mfcc3.read()).decode('utf-8')\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "      \n",
    "\n",
    "       \n",
    "        result = analyze_audio(audio_path)  # Single value result for audio\n",
    "        metadata=analyze_wav_metadata(audio_path)\n",
    "\n",
    "    return {\"result\": result, \"random_array\": random_array,\"metadata\": metadata,  \"prediction\": prediction,\n",
    "            \"frame_base64\": frame_base64,\n",
    "            \"dct_base64\": dct_base64,\"image_base64\":image_base64,\"total_blinks\":total_blinks,\"irregular_blinks\":irregular_blinks,\"full_prediction_string\":full_prediction_string,\"transcribed_text\": transcribed_text,\"similarity\":similarity ,\"micro\":micro,\"freq\":freq,\n",
    "           \"gaze\":gaze,\"lip\":lip,\"mfcc1_64\":mfcc1_64,\"mfcc2_64\":mfcc2_64,\"mfcc3_64\":mfcc3_64}\n",
    "\n",
    "# Run the FastAPI app\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb2e17-5f20-447d-86b7-5ce834b2784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import SpatialDropout3D, Input, Conv3D, BatchNormalization, Activation, MaxPooling3D, Bidirectional, LSTM, Dense, TimeDistributed, ZeroPadding3D, Flatten, Dropout, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "# Load dLib's pre-trained face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def load_video(path: str) -> List[float]:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    target_size = (60, 40)  # Define a fixed size for the cropped lip region (height, width)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale (dLib expects grayscale images)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = detector(gray_frame)\n",
    "\n",
    "        for face in faces:\n",
    "            # Get the landmarks/parts for the face\n",
    "            landmarks = predictor(gray_frame, face)\n",
    "\n",
    "            # Extract the coordinates of the lips (points 48-67)\n",
    "            lip_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)]\n",
    "\n",
    "            # Create a bounding box around the lips\n",
    "            x_coords, y_coords = zip(*lip_points)\n",
    "            min_x, max_x = min(x_coords), max(x_coords)\n",
    "            min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "            # Crop the lip region from the frame\n",
    "            lip_region = frame[min_y:max_y, min_x:max_x]\n",
    "\n",
    "            # Resize the lip region to the target size\n",
    "            lip_region_resized = cv2.resize(lip_region, target_size)\n",
    "\n",
    "            # Convert to grayscale and append to frames\n",
    "            lip_region_gray = tf.image.rgb_to_grayscale(lip_region_resized)\n",
    "            frames.append(lip_region_gray)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames - mean), tf.float32) / std\n",
    "\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(path: str):\n",
    "    path = bytes.decode(path.numpy())\n",
    "    frames = load_video(path)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def mappable_function(path:str) ->List[str]:\n",
    "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
    "    return result\n",
    "\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "inputs = Input(shape=(75, 40, 60, 1))\n",
    "\n",
    "    # Convolutional layers with BatchNormalization and SpatialDropout3D\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(inputs)\n",
    "x = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(x)\n",
    "x = Activation('relu', name='actv1')(x)\n",
    "x = BatchNormalization(name='batc1')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_1')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(x)\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(x)\n",
    "x = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(x)\n",
    "x = Activation('relu', name='actv2')(x)\n",
    "x = BatchNormalization(name='batc2')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_2')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(x)\n",
    "x = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(x)\n",
    "x = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(x)\n",
    "x = Activation('relu', name='actv3')(x)    \n",
    "x = BatchNormalization(name='batc3')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_3')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(x)\n",
    "\n",
    "    # Reshape for RNN layers\n",
    "x = TimeDistributed(Reshape((-1,)), name='time_distributed_1')(x)\n",
    "    # RNN layers\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal  , name='gru1'), merge_mode='concat')(x)\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal , name='gru2'), merge_mode='concat')(x)\n",
    "\n",
    "    # Dense and Activation layers\n",
    "x = Dense(41, kernel_initializer='he_normal', name='dense1')(x)\n",
    "x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "model_lip = tf.keras.Model(inputs, x)\n",
    "\n",
    "model_lip.load_weights(\"./models/dlib3_lipnet_model.h5\")\n",
    "model_lip.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)\n",
    "\n",
    "# Example model prediction line\n",
    "def model_predict(sample):\n",
    "    # Assuming your model expects input of shape (1, 75, 40, 60, 1)\n",
    "    yhat = model_lip.predict(tf.expand_dims(sample, axis=0))\n",
    "    return yhat\n",
    "\n",
    "# Function to split video into chunks of (75, 40, 60, 1)\n",
    "def split_video_into_chunks(video, chunk_size=75):\n",
    "    \"\"\"\n",
    "    Splits the video into chunks of shape (75, 40, 60, 1).\n",
    "    If the last chunk has fewer than 75 frames, it will be padded with zeros.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_frames = video.shape[0]  # The first dimension is the number of frames (x)\n",
    "\n",
    "    # Iterate through the video in steps of 75 frames\n",
    "    for i in range(0, total_frames, chunk_size):\n",
    "        chunk = video[i:i + chunk_size]\n",
    "        \n",
    "        # If the chunk has fewer than 75 frames, pad it\n",
    "        if chunk.shape[0] < chunk_size:\n",
    "            padding = np.zeros((chunk_size - chunk.shape[0], 40, 60, 1))\n",
    "            chunk = np.concatenate((chunk, padding), axis=0)\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Function to process video through the model\n",
    "def process_video_through_model(video):\n",
    "    \"\"\"\n",
    "    Takes a video of shape (x, 40, 60, 1) where x is the number of frames, splits it into\n",
    "    chunks of shape (75, 40, 60, 1), and runs each chunk through the model.\n",
    "    \"\"\"\n",
    "    # Split the video into chunks of (75, 40, 60, 1)\n",
    "    video_chunks = split_video_into_chunks(video)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Run each chunk through the model\n",
    "    for chunk in video_chunks:\n",
    "        print(chunk.shape)\n",
    "        yhat = model_lip.predict(tf.expand_dims(chunk, axis=0))\n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()\n",
    "        tensor=[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]\n",
    "        string_value = tensor[0].numpy().decode(\"utf-8\")\n",
    "        predictions.append(string_value)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Example usage\n",
    "# Assume your video is of shape (x, 40, 60, 1), where x varies\n",
    "video = load_data(tf.convert_to_tensor('uploaded_videos/sgap2p.mp4'))\n",
    "\n",
    "# Process the video through the model\n",
    "predictions = process_video_through_model(video)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71007d-efa7-47ec-aa87-57a84c119f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2968f-8b5a-4605-a013-37d158f89588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    "def convert_video_to_wav(video_file, output_audio=\"extracted_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Convert a video file to .wav audio format using moviepy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the video file\n",
    "        video = VideoFileClip(video_file)\n",
    "\n",
    "        # Extract audio and write it to a .wav file\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(output_audio, codec='pcm_s16le')  # 'pcm_s16le' for .wav format\n",
    "        print(f\"Audio extracted and saved as {output_audio}\")\n",
    "        return output_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video to audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    \"\"\"\n",
    "    Convert an audio file (wav format) to text using Google Speech Recognition.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            print(\"Recognizing audio...\")\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "            # Convert speech to text using Google's Speech-to-Text\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(\"Transcribed Text: \", text)\n",
    "            return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error with Google Speech Recognition service: {e}\")\n",
    "        return None\n",
    "\n",
    "def video_to_text(video_file):\n",
    "    \"\"\"\n",
    "    Complete process: Convert a video file to text by extracting audio and\n",
    "    applying speech-to-text conversion.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the video to wav format\n",
    "    audio_file = convert_video_to_wav(video_file)\n",
    "\n",
    "    if audio_file:\n",
    "        # Step 2: Convert the extracted audio to text\n",
    "        text = audio_to_text(audio_file)\n",
    "        return text\n",
    "    else:\n",
    "        print(\"Audio extraction failed.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "video_path = \"uploaded_videos/sgap2p.mp4\"  # Replace with the path to your video\n",
    "transcribed_text = video_to_text(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6168868-3c99-4af7-9916-f52a82b26577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import speech_recognition as sr\n",
    "\n",
    "def convert_video_to_wav(video_file, output_audio=\"extracted_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Convert a video file to .wav audio format using pydub.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the video file and extract audio\n",
    "        audio = AudioSegment.from_file(video_file)\n",
    "        # Save audio to .wav file\n",
    "        audio.export(output_audio, format=\"wav\")\n",
    "        print(f\"Audio extracted and saved as {output_audio}\")\n",
    "        return output_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video to audio: {e}\")\n",
    "        return None\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    \"\"\"\n",
    "    Convert an audio file (wav format) to text using Google Speech Recognition.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            print(\"Recognizing audio...\")\n",
    "            audio_data = recognizer.record(source)\n",
    "\n",
    "            # Convert speech to text using Google's Speech-to-Text\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(\"Transcribed Text: \", text)\n",
    "            return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error with Google Speech Recognition service: {e}\")\n",
    "        return None\n",
    "\n",
    "def video_to_text(video_file):\n",
    "    \"\"\"\n",
    "    Complete process: Convert a video file to text by extracting audio and\n",
    "    applying speech-to-text conversion.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the video to wav format\n",
    "    audio_file = convert_video_to_wav(video_file)\n",
    "\n",
    "    if audio_file:\n",
    "        # Step 2: Convert the extracted audio to text\n",
    "        text = audio_to_text(audio_file)\n",
    "        return text\n",
    "    else:\n",
    "        print(\"Audio extraction failed.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "video_path = \"uploaded_videos/sgap2p.mp4\"  # Replace with the path to your video\n",
    "transcribed_text = video_to_text(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca476ed-10cf-4dcd-9849-cadc046f555b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6e0ea-f321-4597-909c-e2df0dd459f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31add346-03a3-4a0b-8fb0-5ccb2398b5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
