{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa20e97-fcc4-4775-a32c-c02f8783ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Model\n",
    "from keras.layers import SpatialDropout3D, Input, Conv3D, BatchNormalization, Activation, MaxPooling3D, Bidirectional, LSTM, Dense, TimeDistributed, ZeroPadding3D, Flatten, Dropout, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "a=\"\"\n",
    "# Load dLib's pre-trained face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def load_video(path: str) -> List[float]:\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    target_size = (60, 40)  # Define a fixed size for the cropped lip region (height, width)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale (dLib expects grayscale images)\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = detector(gray_frame)\n",
    "\n",
    "        for face in faces:\n",
    "            # Get the landmarks/parts for the face\n",
    "            landmarks = predictor(gray_frame, face)\n",
    "\n",
    "            # Extract the coordinates of the lips (points 48-67)\n",
    "            lip_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(48, 68)]\n",
    "\n",
    "            # Create a bounding box around the lips\n",
    "            x_coords, y_coords = zip(*lip_points)\n",
    "            min_x, max_x = min(x_coords), max(x_coords)\n",
    "            min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "            # Crop the lip region from the frame\n",
    "            lip_region = frame[min_y:max_y, min_x:max_x]\n",
    "\n",
    "            # Resize the lip region to the target size\n",
    "            lip_region_resized = cv2.resize(lip_region, target_size)\n",
    "\n",
    "            # Convert to grayscale and append to frames\n",
    "            lip_region_gray = tf.image.rgb_to_grayscale(lip_region_resized)\n",
    "            frames.append(lip_region_gray)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    return tf.cast((frames - mean), tf.float32) / std\n",
    "\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(path: str):\n",
    "    path = bytes.decode(path.numpy())\n",
    "    frames = load_video(path)\n",
    "\n",
    "    return frames\n",
    "\n",
    "def mappable_function(path:str) ->List[str]:\n",
    "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
    "    return result\n",
    "\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss\n",
    "\n",
    "\n",
    "inputs = Input(shape=(75, 40, 60, 1))\n",
    "\n",
    "    # Convolutional layers with BatchNormalization and SpatialDropout3D\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(inputs)\n",
    "x = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(x)\n",
    "x = Activation('relu', name='actv1')(x)\n",
    "x = BatchNormalization(name='batc1')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_1')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(x)\n",
    "x = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(x)\n",
    "x = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(x)\n",
    "x = Activation('relu', name='actv2')(x)\n",
    "x = BatchNormalization(name='batc2')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_2')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(x)\n",
    "x = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(x)\n",
    "x = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(x)\n",
    "x = Activation('relu', name='actv3')(x)    \n",
    "x = BatchNormalization(name='batc3')(x)\n",
    "x = SpatialDropout3D(0.5, name='spatial_dropout3d_3')(x)\n",
    "x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(x)\n",
    "\n",
    "    # Reshape for RNN layers\n",
    "x = TimeDistributed(Reshape((-1,)), name='time_distributed_1')(x)\n",
    "    # RNN layers\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal  , name='gru1'), merge_mode='concat')(x)\n",
    "x = Bidirectional(GRU(256, return_sequences=True, kernel_initializer=tf.keras.initializers.Orthogonal , name='gru2'), merge_mode='concat')(x)\n",
    "\n",
    "    # Dense and Activation layers\n",
    "x = Dense(41, kernel_initializer='he_normal', name='dense1')(x)\n",
    "x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "model_lip = tf.keras.Model(inputs, x)\n",
    "\n",
    "model_lip.load_weights(\"./models/dlib3_lipnet_model.h5\")\n",
    "model_lip.compile(optimizer=Adam(learning_rate=0.0001), loss=CTCLoss)\n",
    "\n",
    "# Example model prediction line\n",
    "def model_predict(sample):\n",
    "    # Assuming your model expects input of shape (1, 75, 40, 60, 1)\n",
    "    yhat = model_lip.predict(tf.expand_dims(sample, axis=0))\n",
    "    return yhat\n",
    "\n",
    "# Function to split video into chunks of (75, 40, 60, 1)\n",
    "def split_video_into_chunks(video, chunk_size=75):\n",
    "    \"\"\"\n",
    "    Splits the video into chunks of shape (75, 40, 60, 1).\n",
    "    If the last chunk has fewer than 75 frames, it will be padded with zeros.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    total_frames = video.shape[0]  # The first dimension is the number of frames (x)\n",
    "\n",
    "    # Iterate through the video in steps of 75 frames\n",
    "    for i in range(0, total_frames, chunk_size):\n",
    "        chunk = video[i:i + chunk_size]\n",
    "        \n",
    "        # If the chunk has fewer than 75 frames, pad it\n",
    "        if chunk.shape[0] < chunk_size:\n",
    "            padding = np.zeros((chunk_size - chunk.shape[0], 40, 60, 1))\n",
    "            chunk = np.concatenate((chunk, padding), axis=0)\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return np.array(chunks)\n",
    "\n",
    "# Function to process video through the model\n",
    "def process_video_through_model(video):\n",
    "    \"\"\"\n",
    "    Takes a video of shape (x, 40, 60, 1) where x is the number of frames, splits it into\n",
    "    chunks of shape (75, 40, 60, 1), and runs each chunk through the model.\n",
    "    \"\"\"\n",
    "    # Split the video into chunks of (75, 40, 60, 1)\n",
    "    video_chunks = split_video_into_chunks(video)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Run each chunk through the model\n",
    "    for chunk in video_chunks:\n",
    "        print(chunk.shape)\n",
    "        yhat = model_lip.predict(tf.expand_dims(chunk, axis=0))\n",
    "        decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()\n",
    "        tensor = [tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]\n",
    "        string_value = tensor[0].numpy().decode(\"utf-8\")\n",
    "        predictions.append(string_value)\n",
    "    \n",
    "    # Concatenate all predictions into a single string\n",
    "    full_prediction_string = ' '.join(predictions)\n",
    "    \n",
    "    return full_prediction_string\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
